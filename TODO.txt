### Fixing data sets

* We need to add a dataset which corresponds to the polls dataset which is used in the primer. We will need to add some "fake" variables to this data set, but we will clearly label them as such in the help page.

* We need to expand the datasets which are used in the primer so that each has:
  
  + at least one character variable which takes on exactly two values.
  + at least one character variable which takes on at least 3, but no more than 6 values.
  + at least one numeric values with a non-absurd distribution
  + at least one character variable with two values which might plausibly be discussed as a treatment variable, i.e., something which, at least in theory, we might manipulate. This might need to be fake, but will clearly be noted as such on the help page.

* These are the data sets which should be fixed: polls, trains,  stops






### Stepher

* Read about data and R packages: https://r-pkgs.org/data.html

* Look at stops and trains. Understand how they are produced. Recreate them by hand. Do not submit a PR. Look at data-raw/*R file. Load up data set interactively and look at it. Look at R/*R file. Mess around. Change the trains.R file. Create new Rd with devtools::document(). Confirm the change by running devtools::install and the loading the package and doing ?trains. You can mess around because, at the end, you are just going to discord the changes anyway.

* Let's do something with Census blocks!

* There is a package called PPBDS/tidycensus.tutorials. It connects to this book: https://walker-data.com/census-r/. Do (quickly, if you like) the tutorials for chapters 2 through 5.


* We want a data set which is built at the lowest level, presumably Census blocks. It should have several different variables in it. I am flexible about which variables we will include. We will have have variables at "higher" levels. For example, we will have a `state` column in the data.

* Create a script, get_census.R, which grabs this data. Get at least 4 or 5 variables. Store it in data-raw. Example:

library(tidycensus)
library(tidyverse)

x <- tidycensus::get_decennial(
  geography = "block",
  variables = "H1_001N",
  state = "OK",
  county = "Cimarron",
  year = 2020,
  sumfile = "pl"
)

write_csv(x, file = "census.csv")

It is useful to get this working for just a one county or one state before you try to download all the data for the entire US.

* Script results in a census.csv.

* Create make_census.R script which takes csv file, does a bunch of cleaning, and then outputs an R object, just like the others in the package.

* Key question: Which variables??? Census is overwhelming. Census is hard to use. Find a cool plot and get the variables for that!

### Other Stuff







### Improvements

* Drop some data, like names.csv, which we no longer use for the class.

* Add some data for class examples, like resume.csv

* Revisit the connectedness data.
  - We have copies of the NYT article in the bootcamp repo. Where should we keep that so that students have easy access?
  - Would be nice to add income data so that we can reproduce some of the scatter plots from that article.
  - Might be nice to add some other variables. Even other versions of connectedness would allow us to test the robustness of our main finding.


* Look at: https://upworthy.natematias.com/. Pick one data set which you think is cool. Add it to the package. Think about stuff which might be valuable in the chapters, obviously.


* Consider deleting unused data

* Add the data for individual questions to the train data. That means adding a column for "question", with settings which include "overall". Or maybe we need two train data sets: the current one and trains_detailed?

* Learn about making test cases from [R Packages](https://r-pkgs.org/tests.html) and add at least one test case for each data set. (Be smart in making these. It is OK to have a test case which tests the number of rows in trains because trains will always (?) have 115 rows. It would be bad (or maybe not?) to have a test case like that for cces since the number of rows for will change once we add next year's data to it.) The definition of a "careful tour" is that you have looked at make_* for a data set, regenerated it, and added at least two test cases about it. You should certainly use any current stopifnot() as test cases and then remove them from the make_* files.

* Transform all the make_ scripts into functions --- I think this is more professional --- which, by default, just return the data. There is an argument for having these functions also install the new data in the package, but the valaue of that argument  is false by default.

* Should we re-create all the data sets each time we run R CMD package check? This would not take too much time and would ensure that everything is ship-shape.

* pkgdown

  - Look for nice themes or yaml options

  - Nicer graphics on home page.

  - Instead, we could have an "Article" which provided a list of all the tibbles along with the one paragraph description. Then, the front page could just have a basic blurb and one nice graphic.

* Create better stickers with hexSticker package.


### Data Sets

* Need some business data sets, especially ones with real randomized testing. Maybe A/B testing of some sort.

* Non-political science data:

 - NYC taxi and limousine trip records (https://registry.opendata.aws/nyc-tlc-trip-records-pds/)
 - College major & income (https://github.com/rfordatascience/tidytuesday/tree/master/data/2018/2018-10-16)
 - Science experiment: https://science.sciencemag.org/content/358/6364/776/tab-figures-data
 - Called `police`. [Open police data](https://openpolicing.stanford.edu/data/). Something with millions of rows and, ideally, geographic coordinates. We won't include this data in our repo. Instead, the make_police.R script will download and process it. Ideally, we can get the final police.rda to be less than 100 meg so that we don't need to turn on git-lfs again.
 - World Values Survey: http://www.worldvaluessurvey.org/wvs.jsp


