
* Get at least one good time series data. Maybe weather? Maybe we gather it ourselves. Maybe Satelite data: https://www.drroyspencer.com/latest-global-temperatures/

Download https://www.nsstc.uah.edu/data/msu/v6.0/tlt/uahncdc_lt_6.0.txt

read_fwf() should work.

* Add data from this page: https://crime-data-explorer.app.cloud.gov/downloads-and-docs. A few ready-to-use data set from this source can also be found here: https://crimebythenumbers.com/. Pick a few interesting variables, ideally murder-related stuff. For several years. With geography. Happy to use Github from crime_by_the_numbers.



### Improvements

* Revisit the R CMD check issue and latex failure. Could also add --no-manual to Project options.

* Create a cool plot for primer.data homepage using primer.data data set. Maybe republican vote by income. Or (better?) ideology by income. Order states in an interesting way.

* Add the data for individual questions to the train data. That means adding a column for "question", with settings which include "overall". Or maybe we need two train data sets: the current one and trains_detailed?

* Learn about making test cases from [R Packages](https://r-pkgs.org/tests.html) and add at least one test case for each data set. (Be smart in making these. It is OK to have a test case which tests the number of rows in trains because trains will always (?) have 115 rows. It would be bad (or maybe not?) to have a test case like that for cces since the number of rows for will change once we add next year's data to it.) The definition of a "careful tour" is that you have looked at make_* for a data set, regenerated it, and added at least two test cases about it. You should certainly use any current stopifnot() as test cases and then remove them from the make_* files.

* Transform all the make_ scripts into functions --- I think this is more professional --- which, by default, just return the data. There is an argument for having these functions also install the new data in the package, but the valaue of that argument  is false by default.

* Should we re-create all the data sets each time we run R CMD package check? This would not take too much time and would ensure that everything is ship-shape.

* pkgdown

  - Look for nice themes or yaml options

  - Nicer graphics on home page.

  - Instead, we could have an "Article" which provided a list of all the tibbles along with the one paragraph description. Then, the front page could just have a basic blurb and one nice graphic.

* After semester: change "Control" to "No Postcard" in shaming$treatment. Makes clear that "control" is a conceptual category. There does not need to be a level called "Control".

* Create better stickers with hexSticker package.



### Data Sets

* Non-political science data:


 - Shark attacks since the 16th century (https://www.floridamuseum.ufl.edu/shark-attacks/)
 - Deadly electorate conflicts worldwide (https://doi.org/10.1177%2F00220027211021620)
 - NYC taxi and limousine trip records (https://registry.opendata.aws/nyc-tlc-trip-records-pds/)
 - Trump's Twitter insults (https://www.kaggle.com/ayushggarg/all-trumps-twitter-insults-20152021)
 - College major & income (https://github.com/rfordatascience/tidytuesday/tree/master/data/2018/2018-10-16)




* Science experiment: https://science.sciencemag.org/content/358/6364/776/tab-figures-data


* Called `police`. [Open police data](https://openpolicing.stanford.edu/data/). Something with millions of rows and, ideally, geographic coordinates. We won't include this data in our repo. Instead, the make_police.R script will download and process it. Ideally, we can get the final police.rda to be less than 100 meg so that we don't need to turn on git-lfs again.

* World Values Survey: http://www.worldvaluessurvey.org/wvs.jsp


